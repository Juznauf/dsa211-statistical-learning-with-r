---
title: "Regularization"
output: html_notebook
---

![](p82.png)




If lambda approaches inf, then the impact of the shrinkage penalty grows, and the ridge regresion coefficient estimates will approach zero. 

Note that in (6.5), the shrinkage penalty is applied to β1,...,βp, but not to the intercept β0. We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when xi1 = xi2 = ... = xip = 0. If we assume that the variables — that is, the columns of the data matrix X — have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form ˆ β0 = ¯ y =Pn i=1 yi/n.



![](p83.png)

### Example of standardize an independent variable 

```{r}
x1 <- c(12, 14 , 15, 18, 20)
y1 <- c(2 , 5 , 6, 7, 10)


out1 <- lm(y1~x1)

d <- sd(x1)
x2 <- x1/d # standardize x1
out2 <- lm(y1~x2)
x1
x2

plot(2:22,2:22, type = "n")
points(x1, y1, col = "red", pch = 24)
points(x2, y1, col = "blue", pch = 23)

summary(out1)
summary(out2)
```

### Standardizing does not change the OLS estimates 

Best to standardize before applying the ridge regression formula such that all predictors are on the same scale




![](p84.png)



![](p85.png)


![](p86.png)


![](p87.png)

For an intermediate value of lambda, the MSE is considerably lower. Possible for ridge test MSE to be higher than OLS MSE at very high values of lambda. Additionaly at high shrinkage, it casuse the cefficients to significantly underestimate, resulting in a large increase in the bias and hence the test MSE.



### Numerical Example: Ridge regression 

```{r}
library(leaps)
library(glmnet)
```

```{r}
Y <- c(2.46, 2.74, 3.02, 2.94, 3.21, 3.17, 3.42, 3.67)
X1 <- c(0.51, 1.01, 1.52, 1.78, 2.02, 2.28, 3.03, 3.54)
X2 <- c(1.14, 2.4, 1.14, 3.42, 3.81, 2.67, 2.09, 1.52)
lassodata <- data.frame(Y, X1, X2)
res <- lm(Y~., data=lassodata)
summary(res)
norm2 <- sqrt(res$coefficients[2]^2+res$coefficients[3]^2) # calculate the norm 2
x <- model.matrix(Y~., lassodata)[,-1]  # remove Y variable
y <- lassodata$Y
grid <- seq(15, 0, length=16)  # set lambda values
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)  #ridge regression uses alpha=0, lasso uses alpha=1
```


```{r}
l2norm <- rep(NA, 16) 
# dim(coef(ridge.mod))  # dimension is 3 times 16
# calculate the l2 norm ratio for each lambda
for (i in 1:16) 
  l2norm[i] <- sqrt(sum(coef(ridge.mod)[-1,i]^2))/norm2  # sum of square except for b0
ridge.mod$lambda[1]  #lambda=15
coef(ridge.mod)[,1]
l2norm[1]
ridge.mod$lambda[11]  #lambda=5
coef(ridge.mod)[,11]
l2norm[11]
ridge.mod$lambda[16] #lambda=0
coef(ridge.mod)[,16]  # will be the same as the OLS coef
l2norm[16] 
plot(ridge.mod$lambda, coef(ridge.mod)[2,], main="Ridge Reg: lambda versus coef related to X1", xlab="lambda", ylab="estimated coef related to X1", type="p")
plot(ridge.mod$lambda, coef(ridge.mod)[3,], main="Ridge Reg: lambda versus coef related to X2", xlab="lambda", ylab="estimated coef related to X2", type="p")
plot(l2norm, coef(ridge.mod)[2,], main="Ridge Reg: l2norm versus coef related to X1",
     xlab="l2norm", ylab="estimated coef related to X1", type="p")
plot(l2norm, coef(ridge.mod)[3,], main="Ridge Reg: l2norm versus coef related to X2",
     xlab="l2norm", ylab="estimated coef related to X2", type="p")
```



![](p88.png)

![](p89.png)

![](p90.png)


![](p91.png)


### Lasso Numerical Example

```{r}
res <- lm(Y~., data=lassodata)
summary(res)
norm1 <- sum(abs(res$coefficients[c(2,3)])) # calculate the norm 1 under least squared method
norm1
x <- model.matrix(Y~., lassodata)[,-1]  # remove Y variable
y <- lassodata$Y
grid <- seq(0.38, 0, length=20)  # set lambda values
ridge.mod <- glmnet(x, y, alpha=1, lambda=grid)  
l1norm <- rep(NA, 20) 
# dim(coef(ridge.mod))
```


```{r}
# calculate the l1 norm ratio for each lambda
for (i in 1:20) 
  l1norm[i] <- sum(abs(coef(ridge.mod)[c(2,3),i]))/norm1
ridge.mod$lambda[1]
coef(ridge.mod)[,1]
l1norm[1]
ridge.mod$lambda[10]
coef(ridge.mod)[,10]
l1norm[10]
ridge.mod$lambda[20]
coef(ridge.mod)[,20]
l1norm[20]
plot(ridge.mod$lambda, coef(ridge.mod)[2,], main="Lasso: lambda versus coef related to X1", xlab="lambda", ylab="estimated coef related to X1", type="p")
plot(ridge.mod$lambda, coef(ridge.mod)[3,], main="Lasso: lambda versus coef related to X2", xlab="lambda", ylab="estimated coef related to X2", type="p")
plot(l1norm, coef(ridge.mod)[2,], main="Lasso: l1norm versus coef related to X1",
     xlab="l1norm", ylab="estimated coef related to X1", type="p")
plot(l1norm, coef(ridge.mod)[3,], main="Lasso: l1norm versus coef related to X2",
     xlab="l1norm", ylab="estimated coef related to X2", type="p")

```


![](p92.png)


![](p93.png)

![](p94.png)



### Ridge Regression example with many different lambdas

```{r}
library(ISLR)
library(leaps)
library(glmnet)
RNGkind(sample.kind = "Rounding")
```

```{r}
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hit <- na.omit(Hitters)
dim(Hit)
sum(is.na(Hit))
x <- model.matrix(Salary~., Hit)[,-1]
y <- Hit$Salary
grid <- 10^seq(10, -2, length=100)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2) # get half of data as training set
test <- (-train)  # the rest as testing set
y.test <- y[test]
y.train <- y[train]
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred4 <- predict(ridge.mod, s=4, newx=x[test,])  # if lambda=4, obtain the predict value
mean((ridge.pred4-y.test)^2)     # get the MSE
ridge.pred0 <- predict(ridge.mod, s=0, newx=x[test,]) # use least square prediction
mean((ridge.pred0-y.test)^2)

```




### Ridge regression find lambda using CV

```{r}
set.seed(1)
cvrr.out <- cv.glmnet(x[train,], y[train], alpha=0) #obtain the 10-fold cross validation error
plot(cvrr.out) 
bestlam <- cvrr.out$lambda.min  #identify the lambda for smallest CV error
bestlam
ridge.pred <-predict(ridge.mod, s=bestlam, newx=x[test,])  # get the prediction from best lambda
mean((ridge.pred-y.test)^2)  # obtain the MSE from the test set
out.rr <- glmnet(x,y,alpha=0) # use all the data to refit the model
# get the final model with y-intercept and 19 coefficients by using the best lambda
predict(out.rr, type="coefficients", s=bestlam)[1:20] 

```


### Lasso with cross validation, change alpha to 1

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
out.lasso <- glmnet(x,y, alpha=1, lambda=grid)
lasso.coef <- predict(out.lasso, type="coefficients", s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]  # compare those predictors that are left out
```

For the L1 norm against coefficients we can see that as L1 norm decreases (lambda is increasing), some of the coefficients are shrunked to zero


![](p95.png)




## Homework 9


### Question 1
Based on the College data set, predict the number of applications received, Apps, using the other variables as independent variables. Randomly sample (with set.seed 169) 500 observations from the data set as a training set and the rest observations as a test set.

(a) Fit a multiple linear regression model using least squares approach on the training set. Report the coeﬃcient estimates of the training model and the test error obtained

```{r message=FALSE, warning=FALSE}
set.seed(169)
attach(College) 
set.seed(169) 
# sum(is.na(College))  # to check whether NA exists
# dim(College)        # to check the sample size and number of variables 
train <- sample(1:nrow(College), 500) 
test <- -train 
College.train <- College[train,] 
College.test <- College[test,] 
lm.fit <- lm(Apps~., data=College.train) 
lm.pred <- predict(lm.fit, College.test) 
mean((College.test$Apps-lm.pred)^2) 
lm.fit$coefficients 

```


(b) Fit a ridge regression model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the coeﬃcient estimates of the ﬁnal model. 

```{r}
library(glmnet) 
train.x <- model.matrix(Apps~., data=College.train)[,-1] #take away the intercept
train.y <-College.train$Apps 
test.x <- model.matrix(Apps~., data=College.test)[,-1] 
test.y <- College.test$Apps 
 
grid <- 10^seq(10, -2, length=100) 
ridge.mod <- cv.glmnet(train.x, train.y, alpha=0, lambda=grid) 
lambda.rr <- ridge.mod$lambda.min  
lambda.rr 
ridge.pred <- predict(ridge.mod, newx=test.x, s=lambda.rr) 
mean((test.y-ridge.pred)^2) 
 
x <- model.matrix(Apps~., data=College)[,-1] 
y <- College$Apps 
out.rr <- glmnet(x,y, alpha=0, lambda=grid) 
rr.coef <- predict(out.rr, type="coefficients", s=lambda.rr)[1:18,] 
rr.coef 
```


The steps are to first create the test and train matrix as well as the matrix of the entire dataset. 

use cross validation on the training set to get the best lambda

find the test error by using the best lambda and the test set

fit a new ridge regression using all of the available data and the grid lambda

predict the coefficients of the ridge regression with the best lambda



(c) Fit a lasso model on the training set, with λ chosen by cross-validation. Report the test error obtained, along with the coeﬃcient estimates of the ﬁnal model.


```{r}
lasso.mod <- cv.glmnet(train.x, train.y, alpha=1, lambda=grid) 
lambda.lasso <- lasso.mod$lambda.min 
lambda.lasso 
lasso.pred <- predict(lasso.mod, newx=test.x, s=lambda.lasso) 
mean((test.y-lasso.pred)^2) 
 
out.lasso <- glmnet(x,y, alpha=1, lambda=grid) 
lasso.coef <- predict(out.lasso, type="coefficients", s=lambda.lasso)[1:18,] 
lasso.coef[lasso.coef !=0] 
```


![](p96.png)


