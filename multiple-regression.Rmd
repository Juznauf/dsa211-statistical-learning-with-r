---
title: "Multiple Regression"
output: html_notebook
---



![](p27.png)

### Advertising Example

```{r}
adv <- read.csv("D:/User/Documents/SMU CONTENT/Year 3 Sem 2/DSA 211/week5/Advertising1.csv")
summary(adv)
pairs(adv[-1])
```


![](p29.png)


![](p30.png)



### F-test for overall significance



![](p30.png)

```{r}
lm.sale1 <- lm(sales~TV+radio+newspaper, data  = adv) # or we can write sales~.-X
summary(lm.sale1)


sum1 <- summary(lm.sale1) # can assign the summary so that we can call it in the future
```




![](p31.png)

*Question: is there a relationship between model complexity and variance*

![](p32.png)



![](p33.png)

![](p34.png)

![](p35.png)

*Question: What if the objective is predictive performance? would the hierarchy principle still count*


### Advertising example after deletin newspaper variable

```{r}
lm.sale4 <- lm(sales~TV*radio, data = adv)
summary(lm.sale4)
```


Checking for assumptions of the mul reg model 


```{r message=FALSE, warning=FALSE}
plot(lm.sale4$fitted.values, residuals(lm.sale4), main="Relationship between predicted sales and residuals", xlab="sales", ylab="residuals") 
plot(adv$TV, residuals(lm.sale4), main="Relationship between TV and 
     residuals", xlab="TV", ylab="residuals")
plot(adv$radio, residuals(lm.sale4), main="Relationship between radio and 
     residuals", xlab="radio", ylab="residuals")
plot(adv$TV*adv$radio, residuals(lm.sale4), main="Relationship between interaction and residuals", xlab="TV*radio", ylab="residuals")
library(fitdistrplus)
fnorm <- fitdist(residuals(lm.sale4), distr="norm")
summary(fnorm)

plot(fnorm)
```

*Question: Should we use the KS test for checking of the normality of residuals*


For the plot of fitted values against the residuals, there should be no relationship



Confidence interval of the population slopes
Confidence interval of the dependent variable
Predicition interval of the mean of dependent variable

```{r}

confint(lm.sale4, level=0.95)
predict(lm.sale4, data.frame(TV=20, radio=15), interval="confidence", level=0.95)
predict(lm.sale4, data.frame(TV=20, radio=15), interval="prediction", level=0.95)

```

### Differences between the CI for E(y) and the PI for y given x

- A prediction interval is similar in spirit to a confidence interval, except that 
  * the prediction interval is designed to cover a “moving target”,the random future value of y, while
  * the confidence interval is designed to cover the “fixed target”,the average (expected) value of y, E(y), for a given x*
  
  
- Although both are centered at yˆ, the prediction interval is wider than the confidence interval, for a given x and confidence level.This makes sense, since
  * the prediction interval must take account of the tendency of y to
fluctuate from its mean value, while
  * the confidence interval simply needs to account for the
uncertainty in estimating the mean value.


### Similarities between the CI for E(y) and the PI for y given x


- For a given data set, the error in estimating E(y) and yˆ grows as
x* moves away from x¯. Thus, the further x* is from x¯, the wider
the confidence and prediction intervals will be.
- If any of the conditions underlying the model are violated, then
the confidence intervals and prediction intervals may be invalid
as well. This is why it’s so important to check the conditions by
examining the residuals, etc.




![](p36.png)


### Dummy variable example with the Carseats Data Set

```{r warning=FALSE, message=FALSE}
library(ISLR)
attach(Carseats)
names(Carseats)
summary(Carseats)
pairs(Carseats[-c(dim(Carseats)[2],dim(Carseats)[2]-1,7)])# check for collinearity all except for the dummy variables
```



```{r}
boxplot(Sales ~ShelveLoc, main="Relationship of sales and shelf locations",
        xlab="shelf locations", ylab="sales", col="yellow")
boxplot(Sales ~Urban,  main="Relationship of sales and Urban factor",
        xlab="Urban factor", ylab="sales", col="red")
```




### Multiple regression including all the dummy variables

```{r}
lm.carseat1 <- lm(Sales~., data = Carseats)
summary(lm.carseat1)
```


### Multiple regression with significant factors


```{r}

lm.carseat2 <- lm(Sales~CompPrice + Income + Advertising + Price + ShelveLoc
                  + Age,data = Carseats)
summary(lm.carseat2)
```


### trying the contrast function

```{r}
contrasts(Carseats$ShelveLoc) # to check r encoding for dummy variables
```


### Finding confidence and prediction intervals

```{r}
confint(lm.carseat2, level=0.95)
predict(lm.carseat2, data.frame(CompPrice=116, Income=80, Advertising=7,Price=100, ShelveLoc="Good", Age=56), interval="confidence",level=.95)
predict(lm.carseat2, data.frame(CompPrice=116, Income=80, Advertising=7,Price=100, ShelveLoc="Good", Age=56), interval="prediction",level=.95)
```

# Important when running prediction function make sure that the original lm call does not use negation but instead has only the intended vairables


![](p37.png)

### Mul reg with interaction between Price and shelf location

```{r}
lm.carseat3=lm(Sales~CompPrice+Income+Advertising+Age+Price*ShelveLoc, data=Carseats)

summary(lm.carseat3)
```

### multiple regression model with interaction between Income and Advertising

```{r}
lm.carseat4=lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+Income:Advertising, data=Carseats)
summary(lm.carseat4)

```


![](p38.png)


### Quadratic relationship

```{r}

lm.carseat5=lm(Sales~CompPrice+Income+Advertising+
                 ShelveLoc+Age+poly(Price,2), data=Carseats)
summary(lm.carseat5)


```

```{r}
lm.carseat5=lm(Sales~CompPrice+Income+Advertising+
                 Price+ShelveLoc+Age+I(Price^2), data=Carseats)
summary(lm.carseat5)
```


## Homework 5


### Question 1

A ﬁnancial analyst engaged in business valuation obtained ﬁnancial data on 72 drug companies to predict the price-to-book value. The Business.csv contains the following variables:
PB Ratio: Price-to-book value ratio 
ROE: Return on equity (in %) 
Growth: Growth rate Local: Whether a local or international company (Yes or No) Size: Size of company (Small, Medium or Large)


(a) Find the multiple regression equation with all the four main independent variables. 


```{r}
Business <- read.csv("D:/User/Documents/SMU CONTENT/Year 3 Sem 2/DSA 211/week5/Business.csv")
names(Business)
lm.fit1 <- lm(PB.Ratio~. ,data = Business)
coef(lm.fit1)
```

PB.Ratio= 5.67+0.045ROE+0.0003645SGrowth+0.4777LocalYes+0.1040SizeMedium+0.136SizeSmall 


(b) Determine whether there is a signiﬁcant linear relationship between the price-tobook value ratio and the four independent variables at the 0.05 levelof signiﬁcance. 

```{r}
summary(lm.fit1)
lm.fit1.summary <- summary(lm.fit1)
```

Since p-value=2.2e-16, reject null hypothesis.  There is a significant linear relationship. 



(c) Determine whether each independent variable makes a signiﬁcant contribution to the regression model at the 0.05 level of signiﬁcance. 

```{r}
summary(lm.fit1)
```

only ROE and LocalYes make significant contribution. 


(d) Find the multiple regression equation with only two main independent variables: ROE and Local. 

```{r}
lm.fit2 <- lm(PB.Ratio~ ROE + Local, data = Business)
coef(lm.fit2)
lm.fit2.summary <- summary(lm.fit2)
```

PB.Ratio=5.7606+0.04501ROE+0.4903LocalYes 


(e) Find the multiple regression equation with only two main independent variables: ROE and Local, with their interaction eﬀect. 

```{r}
lm.fit3 <- lm(PB.Ratio~ ROE*Local, data = Business)
coef(lm.fit3)
lm.fit3.summary <- summary(lm.fit3)
```

PB.Ratio=5.6365+0.0467ROE+0.3541LocalYes-0.00897ROE*LocalYes 


(f) When comparing three estimated models in parts (a), (d) and (e), explain the reason why model in part (e) is the best.

```{r}
lm.fit1.summary$adj.r.squared
lm.fit2.summary$adj.r.squared
lm.fit3.summary$adj.r.squared
```

Model 3 is the best as it has the highest adj r squared



(g) Perform a residual analysis on the model in part (e) and determine whether the regression assumptions are valid. (Note that there is no need to plot the interaction eﬀect versus residuals as Local variable is a qualitative variable.) 

```{r , message = FALSE, warning=FALSE}
plot(lm.fit3$fitted.values, residuals(lm.fit3) , main = "Relationship between predicted PB.ratio and residuals", xlab = "Predicted PB.ratio", ylab = "residuals")
plot(Business$ROE, residuals(lm.fit3) , main = "Relationship between ROE and residuals", xlab = "ROE",  ylab = "residuals")

fnorm <- fitdist(residuals(lm.fit3), distr =  "norm")
plot(fnorm)
```



no reason to suspect the assumptions 


(h) Predict the price-to-book value ratio if a local company has the ROE of 24.1% under the model in part (e).


```{r}
predict(lm.fit3, newdata = data.frame(ROE = 24.1, Local  = "Yes"))
```



(i) Construct a 95% prediction interval estimate for the the price-to-book value ratio of an international company if it has the ROE of 24.1% under the model in part (e). 


```{r}
predict(lm.fit3, data.frame(ROE = 24.1, Local = "No"), interval = 'prediction', level = 0.95)
```

(5.766,  7.758) 

(j) Based on the model in part (e), write down the simple linear regression equation for predicting all local companies’ price-to-book value ratio.

```{r}
coef(lm.fit3)

new.intercept <- 0.354087328 + 5.636460792
new.slope <- -0.008970344 + 0.046712677
new.intercept
new.slope
```


The slope = 0.03774233, the intercept = 5.990548

 PB.Ratio=5.9906+0.03773ROE 