---
title: "Linear Model Selection"
output: html_notebook
---


![](\static\p67.png)

![](\static\p68.png)




![](\static\p69.png)


![](\static\p70.png)


![](\static\p71.png)


![](\static\p72.png)


![](\static\p73.png)




![](\static\p74.png)

![](\static\p75.png)


![](\static\p76.png)

![](\static\p77.png)


![](\static\p78.png)


When n is larger than 7, BIC has a heavier penalty compared to cp. Hence BIC will take the simple model


![](\static\p79.png)

### Example removing NA data

```{r}
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hit <- na.omit(Hitters)


dim(Hit)
sum(is.na(Hit))
```


### Leaps library for subset selection

```{r}
# library(leaps)

# best subset

regfit1.all <- regsubsets(Salary~., Hit)
summary(regfit1.all) # will only report up till the 8th variable

```



```{r}
regfit2.all <- regsubsets(Salary~., Hit, nvmax = 19)
reg2.summary <- summary(regfit2.all)
names(reg2.summary) # check what measurements are given in the selection process

```


Plots and identifying the best model using each selection criteria


```{r}
plot(reg2.summary$rss, main="RSS plot", 
     xlab="Number of variables", ylab="RSS", type="b")
plot(reg2.summary$adjr2, main="Adjusted r^2 plot", 
     xlab="Number of variables", ylab="Adjusted r^2", type="b")
plot(reg2.summary$cp, main="Cp plot", 
     xlab="Number of variables", ylab="Cp", type="b")
plot(reg2.summary$bic, main="BIC plot", 
     xlab="Number of variables", ylab="BIC", type="b")
a <- which.min(reg2.summary$rss)
b <- which.max(reg2.summary$adjr2)
c <- which.min(reg2.summary$cp)
d <- which.min(reg2.summary$bic)

```


### After identifying the best model , show the coefficient for each criteria
```{r}

coef(regfit2.all, a)
coef(regfit2.all, b)
coef(regfit2.all, c)
coef(regfit2.all, d)


```


### Backward Stepwise Selection


```{r}
regfit3.all <- regsubsets(Salary~., Hit, nvmax=19, method="backward")
reg3.summary <- summary(regfit3.all)
a <- which.min(reg3.summary$rss)
b <- which.max(reg3.summary$adjr2)
c <- which.min(reg3.summary$cp)
d <- which.min(reg3.summary$bic)
coef(regfit3.all, a)
coef(regfit3.all, b)
coef(regfit3.all, c)
coef(regfit3.all, d)

```


### Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be ﬁt). In contrast, forward stepwise can be used even when n < p, and so is the only viable subset method when p is very large.


### Forward Stepwise Selection

```{r}
regfit4.all <- regsubsets(Salary~., Hit, nvmax=19, method="forward")
reg4.summary <- summary(regfit4.all)
a <- which.min(reg4.summary$rss)
b <- which.max(reg4.summary$adjr2)
c <- which.min(reg4.summary$cp)
d <- which.min(reg4.summary$bic)
coef(regfit4.all, a)
coef(regfit4.all, b)
coef(regfit4.all, c)
coef(regfit4.all, d)

```



![](\static\p80.png)


### Using subset selection with validation

```{r}
RNGkind(sample.kind = "Rounding")

```

```{r}
set.seed(1)
# use validation set approach to select the model

train <- sample(c(TRUE, FALSE), nrow(Hit), rep=TRUE)  # get the random training set
test <- (!train)                # set the rest as test set
regfit22.all <- regsubsets(Salary~., Hit[train,], nvmax=19) #run the best selection on training set

test.mat <- model.matrix(Salary~., Hit[test,])  # make the model matrix from the test data

val.errors <- rep(NA, 19)

# run a loop, and for each value of i, we extract the coefficients from the regfit11.all
# and then multiply the coefficients with the column of the test model matrix to form the predictions
# ans then compute the test MSE

for (i in 1:19) {
  coefi <- coef(regfit22.all, id=i) # extract the coefficients
  pred <- test.mat[, names(coefi)]%*%coefi # multiply the coefficients to form the prediction
  val.errors[i] <- mean((Hit$Salary[test]-pred)^2) # compute the test MSE
}
val.errors


aa <- which.min(val.errors)  # identify i with minimum MSE
aa
coef(regfit22.all, aa)  # the best selected model with coefficients
# But we should use the full data set to get the model!
coef(regfit2.all, aa)  # regfit 2 is the best subset with all 19 models, pick the model with aa complexity





```


![](\static\p81.png)


## We ﬁrst calculate the standard error of the estimated test MSE for each m odel size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model — that is, the model with the smallest number of predictors. In this case, applying the one-standard-error rule to the validation set or cross-validation approach leads to selection of the three-variable model.



## Using the cross validation approach

```{r}
# write a function to do the prediction
predict.regsubsets <- function(object, newdata, id){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}

k <- 10  # determine the k-fold cross-validation
set.seed(1)
folds <- sample(1:k, nrow(Hit), replace=TRUE)
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for (j in 1:k) {
  best.fit <- regsubsets(Salary~., data=Hit[folds!=j,], nvmax=19)
  for (i in 1:19){
    pred <- predict.regsubsets(best.fit, Hit[folds==j,], id=i)
    cv.errors[j,i] <- mean((Hit$Salary[folds==j]-pred)^2)
  }
}
mean.cv <- apply(cv.errors, 2, mean)#to average over the columns of matrix to error for each model
mean.cv
bb <- which.min(mean.cv)
bb

# we should use the full data as training set to get the full model
coef(regfit2.all, bb) # perform best subset selection on the full data set in order to obtain the eleven variable model 

```


### It is important that we make use of the full data set in order to obtain more accurate coeﬃcient estimates. Note that we perform best subset selection on the full data set and select the best tenvariable model, rather than simply using the variables that were obtained from the training set, because the best ten-variable model on the full data set may diﬀer from the corresponding model on the training set.

### In fact, we see that the best ten-variable model on the full data set has a diﬀerent set of variables than the best ten-variable model on the training set. We now try to choose among the models of diﬀerent sizes using crossvalidation. This approach is somewhat involved, as we must perform best subset selection within each of the k training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of k = 10 folds, and we create a matrix in which we will store the results.



## Homework 8

### Question 1

Based on the information in the Credit ﬁle,

(a) Fit the multiple regression equation to predict Balance with all 10 independent variables (note that ID is not a variable) by using the Best Subset Selection with BIC criterion on the training data set. Plot the graph to show the number of variables versus BIC in each selection step. 


```{r}
attach(Credit)
names(Credit)
Cred <- Credit[,-1]
dim(Cred)
dim(Credit)
regfit1.all <- regsubsets(Balance~.,data = Cred, nvmax = 10)
regfit1.all.summary <- summary(regfit1.all)
aa <- which.min(regfit1.all.summary$bic)

plot(regfit1.all.summary$bic, main="BIC plot", 
     xlab="Number of variables", ylab="BIC", type="b")
coef(regfit1.all, aa)
```


Balance=-499.73-7.8392Incom+0.26664Limit+23.175Cards+429.61StudentYes       under the Best Subset Selection 


(b) Fit the multiple regression equation to predict Balance with all 10 independent variables (note that ID is not a variable) by using the Forward Selection with C p criterion on the training data set. Plot the graph to show the number of variables versus Cp in each selection step.


```{r}
regfit2.all <- regsubsets(Balance~.,data = Cred, nvmax = 10, method = 'forward')
regfit2.all.summary <- summary(regfit2.all)
bb <- which.min(regfit2.all.summary$cp)

plot(regfit2.all.summary$bic, main="cp plot", 
     xlab="Number of variables", ylab="cp", type="b")
coef(regfit2.all, bb)

```

Balance=-493.73-7.7951Incom+0.19369Limit+1.0912Rating+18.212Cards0.62405Age+425.61StudentYes 


(c) Fit the multiple regression equation to predict Balance with all 10 independent variables (note that ID is not a variable) by using the Backward Selection with adjusted R2 criterion on the training data set. Plot the graph to show the number of variables versus adjusted R2 in each selection step. 


```{r}


regfit3.all <- regsubsets(Balance~.,data = Cred, nvmax = 10, method = 'backward')
regfit3.all.summary <- summary(regfit3.all)
cc <- which.max(regfit3.all.summary$adjr2)

plot(regfit3.all.summary$adjr2, main="adjr2 plot", 
     xlab="Number of variables", ylab="adjr2", type="b")
coef(regfit3.all, cc)

```


Balance=-488.62-7.8036Incom+0.19362Limit+1.0940Rating+             18.109Cards-0.62065Age-10.453GenderFemale+426.58StudentYes  under the Backward Selection 



(d) Under the validation approach, ﬁt the multiple regression equation to predict Balance with all 10 independent variables (note that ID is not a variable) by using the Best Subset Selection with mean square error criterion on the validation data set. Plot the graph to show the number of variables versus mean square error in each selection step. Use the set.seed(121).


```{r}


set.seed(121)
# use validation set approach to select the model

train <- sample(c(TRUE, FALSE), nrow(Cred), rep=TRUE)  # get the random training set
test <- (!train)                # set the rest as test set
regfit33.all <- regsubsets(Balance~., Cred[train,], nvmax=10) #run the best selection on training set

test.mat <- model.matrix(Balance~., Cred[test,])  # make the model matrix from the test data

val.errors <- rep(NA, 10)

# run a loop, and for each value of i, we extract the coefficients from the regfit11.all
# and then multiply the coefficients with the column of the test model matrix to form the predictions
# ans then compute the test MSE

for (i in 1:10) {
  coefi <- coef(regfit33.all, id=i) # extract the coefficients
  pred <- test.mat[, names(coefi)]%*%coefi # multiply the coefficients to form the prediction
  val.errors[i] <- mean((Cred$Balance[test]-pred)^2) # compute the test MSE
}
val.errors


aa <- which.min(val.errors)  # identify i with minimum MSE
aa
coef(regfit33.all, aa)  # the best selected model with coefficients
# But we should use the full data set to get the model!



```


Balance=-499.73-7.8392Incom+0.26664Limit+23.175Cards+429.61StudentYes






